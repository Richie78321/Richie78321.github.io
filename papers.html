<!DOCTYPE html><html><head><meta name="viewport" content="width=device-width"/><meta charSet="utf-8"/><script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-180638861-1"></script><script>
                  window.dataLayer = window.dataLayer || [];
                  function gtag(){dataLayer.push(arguments);}
                  gtag('js', new Date());
                  gtag('config', 'UA-180638861-1');
              </script><link rel="preconnect" href="https://fonts.gstatic.com"/><link rel="manifest" href="/site.webmanifest"/><link rel="icon" href="/favicon.ico"/><title>Richie Goulazian - Papers</title><meta name="next-head-count" content="9"/><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin /><link rel="preload" href="/_next/static/css/269ac77e95e55796.css" as="style"/><link rel="stylesheet" href="/_next/static/css/269ac77e95e55796.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-5cd94c89d3acac5f.js"></script><script src="/_next/static/chunks/webpack-853d799305da8782.js" defer=""></script><script src="/_next/static/chunks/framework-ff100329a956f214.js" defer=""></script><script src="/_next/static/chunks/main-01df828e572375b9.js" defer=""></script><script src="/_next/static/chunks/pages/_app-593d31cc74f53c80.js" defer=""></script><script src="/_next/static/chunks/556-2de30bf01d6fd0b6.js" defer=""></script><script src="/_next/static/chunks/pages/papers-4c72f40c28931762.js" defer=""></script><script src="/_next/static/m1-HDYOkaNCw9aQ2kPhKg/_buildManifest.js" defer=""></script><script src="/_next/static/m1-HDYOkaNCw9aQ2kPhKg/_ssgManifest.js" defer=""></script><script src="/_next/static/m1-HDYOkaNCw9aQ2kPhKg/_middlewareManifest.js" defer=""></script><style data-href="https://fonts.googleapis.com/css2?family=Roboto:wght@300&display=swap">@font-face{font-family:'Roboto';font-style:normal;font-weight:300;font-display:swap;src:url(https://fonts.gstatic.com/s/roboto/v30/KFOlCnqEu92Fr1MmSU5vAA.woff) format('woff')}@font-face{font-family:'Roboto';font-style:normal;font-weight:300;font-display:swap;src:url(https://fonts.gstatic.com/s/roboto/v30/KFOlCnqEu92Fr1MmSU5fCRc4AMP6lbBP.woff2) format('woff2');unicode-range:U+0460-052F,U+1C80-1C88,U+20B4,U+2DE0-2DFF,U+A640-A69F,U+FE2E-FE2F}@font-face{font-family:'Roboto';font-style:normal;font-weight:300;font-display:swap;src:url(https://fonts.gstatic.com/s/roboto/v30/KFOlCnqEu92Fr1MmSU5fABc4AMP6lbBP.woff2) format('woff2');unicode-range:U+0301,U+0400-045F,U+0490-0491,U+04B0-04B1,U+2116}@font-face{font-family:'Roboto';font-style:normal;font-weight:300;font-display:swap;src:url(https://fonts.gstatic.com/s/roboto/v30/KFOlCnqEu92Fr1MmSU5fCBc4AMP6lbBP.woff2) format('woff2');unicode-range:U+1F00-1FFF}@font-face{font-family:'Roboto';font-style:normal;font-weight:300;font-display:swap;src:url(https://fonts.gstatic.com/s/roboto/v30/KFOlCnqEu92Fr1MmSU5fBxc4AMP6lbBP.woff2) format('woff2');unicode-range:U+0370-03FF}@font-face{font-family:'Roboto';font-style:normal;font-weight:300;font-display:swap;src:url(https://fonts.gstatic.com/s/roboto/v30/KFOlCnqEu92Fr1MmSU5fCxc4AMP6lbBP.woff2) format('woff2');unicode-range:U+0102-0103,U+0110-0111,U+0128-0129,U+0168-0169,U+01A0-01A1,U+01AF-01B0,U+0300-0301,U+0303-0304,U+0308-0309,U+0323,U+0329,U+1EA0-1EF9,U+20AB}@font-face{font-family:'Roboto';font-style:normal;font-weight:300;font-display:swap;src:url(https://fonts.gstatic.com/s/roboto/v30/KFOlCnqEu92Fr1MmSU5fChc4AMP6lbBP.woff2) format('woff2');unicode-range:U+0100-02AF,U+0304,U+0308,U+0329,U+1E00-1E9F,U+1EF2-1EFF,U+2020,U+20A0-20AB,U+20AD-20CF,U+2113,U+2C60-2C7F,U+A720-A7FF}@font-face{font-family:'Roboto';font-style:normal;font-weight:300;font-display:swap;src:url(https://fonts.gstatic.com/s/roboto/v30/KFOlCnqEu92Fr1MmSU5fBBc4AMP6lQ.woff2) format('woff2');unicode-range:U+0000-00FF,U+0131,U+0152-0153,U+02BB-02BC,U+02C6,U+02DA,U+02DC,U+0304,U+0308,U+0329,U+2000-206F,U+2074,U+20AC,U+2122,U+2191,U+2193,U+2212,U+2215,U+FEFF,U+FFFD}</style></head><body><div id="__next" data-reactroot=""><div class="container"><h1 class="display-4 mt-md-5 mt-4 mb-2">Paper List</h1><p class="text-muted mb-3">Below is a list of papers (and other resources) that I found helpful when learning a subject<br/><a class="text-nowrap" href="/"> <!-- -->― Richie Goulazian</a></p><div class="custom-control custom-switch text-muted mb-2"><input type="checkbox" class="custom-control-input" id="showNotesSwitch"/><label class="custom-control-label" for="showNotesSwitch">Show notes</label></div><div class="papersMarkdown"><h2>AI</h2>
<p>Introductory:</p>
<ul>
<li><a href="http://neuralnetworksanddeeplearning.com/chap4.html" target="_blank">A visual proof that neural nets can compute any function</a>
<!-- -->
</li>
</ul>
<p>Transformers:</p>
<ul>
<li><a href="https://arxiv.org/abs/1706.03762" target="_blank">Attention is All You Need</a>
<!-- -->
</li>
<li><a href="https://arxiv.org/abs/2304.10557" target="_blank">An Introduction to Transformers</a></li>
<li><a href="https://arxiv.org/abs/2207.09238" target="_blank">Formal Algorithms for Transfomers</a></li>
</ul>
<p>GPTs:</p>
<ul>
<li><a href="https://www.youtube.com/watch?v=kCc8FmEb1nY" target="_blank">Let&#x27;s build GPT: from scratch, in code, spelled out.</a>
<!-- -->
</li>
<li><a href="https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf" target="_blank">Language Models are Unsupervised Multitask Learners (GPT-2)</a></li>
<li><a href="https://arxiv.org/abs/2005.14165" target="_blank">Language Models are Few-Shot Learners (GPT-3)</a></li>
</ul>
<p>Mixture of Experts (MoE):</p>
<ul>
<li><a href="https://openreview.net/pdf?id=B1ckMDqlg" target="_blank">Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer</a>
<!-- -->
</li>
<li><a href="https://arxiv.org/abs/2101.03961" target="_blank">Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity</a>
<!-- -->
</li>
</ul>
<p>Augmented LLMs:</p>
<ul>
<li><a href="https://arxiv.org/abs/2302.07842" target="_blank">Augmented Language Models: a Survey</a></li>
<li><a href="https://arxiv.org/abs/2312.10997" target="_blank">Retrieval-Augmented Generation for Large Language Models: A Survey</a></li>
<li><a href="https://arxiv.org/abs/2210.03629" target="_blank">ReAct: Synergizing Reasoning and Acting in Language Models</a>
<!-- -->
</li>
</ul>
<p>Test-Time Compute:</p>
<ul>
<li><a href="https://arxiv.org/abs/2305.20050" target="_blank">Let&#x27;s Verify Step by Step</a>
<!-- -->
</li>
</ul>
<p>Model Deployment:</p>
<ul>
<li><a href="https://arxiv.org/abs/2007.03970" target="_blank">Distributed Training of Deep Learning Models:
A Taxonomic Perspective</a>
<!-- -->
</li>
</ul>
<p>Embedding Models:</p>
<ul>
<li><a href="https://arxiv.org/abs/2201.10005" target="_blank">Text and Code Embeddings by Contrastive Pre-Training</a>
<!-- -->
</li>
</ul>
<h2>Distributed Systems</h2>
<ul>
<li><a href="https://static.googleusercontent.com/media/research.google.com/en//archive/spanner-osdi2012.pdf" target="_blank">Spanner: Google’s Globally-Distributed Database</a></li>
<li><a href="https://static.googleusercontent.com/media/research.google.com/en//archive/mapreduce-osdi04.pdf" target="_blank">MapReduce: Simplified Data Processing on Large Clusters</a></li>
<li><a href="https://www.usenix.org/legacy/event/atc10/tech/full_papers/Hunt.pdf" target="_blank">ZooKeeper: Wait-free coordination for Internet-scale systems</a></li>
</ul></div></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"papersMarkdown":"## AI\n\nIntroductory:\n\n- [A visual proof that neural nets can compute any function](http://neuralnetworksanddeeplearning.com/chap4.html)\n  - This interactive website provides a satisfying justification for the \"power\" of neural networks to accomplish almost anything.\n\nTransformers:\n\n- [Attention is All You Need](https://arxiv.org/abs/1706.03762)\n  - The original paper introducing Transformer models, the model used for most LLMs.\n- [An Introduction to Transformers](https://arxiv.org/abs/2304.10557)\n- [Formal Algorithms for Transfomers](https://arxiv.org/abs/2207.09238)\n\nGPTs:\n\n- [Let's build GPT: from scratch, in code, spelled out.](https://www.youtube.com/watch?v=kCc8FmEb1nY)\n  - A truly awesome tutorial that helps build an intuition for how these models are actually \"built\".\n- [Language Models are Unsupervised Multitask Learners (GPT-2)](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)\n- [Language Models are Few-Shot Learners (GPT-3)](https://arxiv.org/abs/2005.14165)\n\nMixture of Experts (MoE):\n\n- [Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer](https://openreview.net/pdf?id=B1ckMDqlg)\n  - This paper explores the use of Mixture of Experts (MoE) for NLP. MoE is rumored to be used in GPT-4.\n  - Mixture of Experts = only running select parts of a neural network during inference time. Each \"part\" of the network is called an expert. The routing is determined by learned parameters, as simple as a single weight matrix.\n  - MoE is nice because it allows for scaling model parameter count without needing to proportionally scale inference compute.\n  - They explored the challenge of splitting inference across experts reducing the number of examples each expert sees within a batch.\n    - Naively, one would train multiple copies of the full model in parallel. This would require `N` times more training data per batch for `N` experts in order to get the same batch size for the expert.\n    - Instead, they only host a _single_ shared instance of an expert, such that increasing the number of distributed devices increases the number of samples seen by the expert.\n  - Something I didn't fully understand in this paper and would like to revisit is how they made the routing differentiable using random noise. It seems that by adding noise, they made routing involve continuous probabilities that enabled differentiation?\n- [Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity](https://arxiv.org/abs/2101.03961)\n  - This paper explores the idea that it is unnecessary to route to more than one expert at a time.\n  - In this case, the MoE layers are in the feedforward section of the transformer layers. The experts operate indepdently on each token in the sequence, as is done with the standard feedforward section of the transformer layer.\n  - This paper also discusses a lot of the infrastructure challenges with this type of model. I have less experience in the domain of distributed training so didn't get as much from this section. One interesting idea was the idea of \"expert capacity\", where experts would process batches of multiple tokens at once, which is necessary to maximize the compute capacity of GPUs/TPUs. Because the number of tokens allocated to an expert within a single batch is dynamic based on the routing gate \u0026 matrix multiplications needed to be statically initialized, the researchers essentially needed to \"predict\" the maximum batch size for each expert.\n\nAugmented LLMs:\n\n- [Augmented Language Models: a Survey](https://arxiv.org/abs/2302.07842)\n- [Retrieval-Augmented Generation for Large Language Models: A Survey](https://arxiv.org/abs/2312.10997)\n- [ReAct: Synergizing Reasoning and Acting in Language Models](https://arxiv.org/abs/2210.03629)\n  - ReAct (reason + action) is an LLM prompting strategy that interleaves thoughts (reasoning) with actions (searching, performing some observation, answering the question) in order to accomplish some task.\n  - ReAct is a combination of two existing approaches: chain-of-thought prompting for improving LLM reasoning + actions.\n\nTest-Time Compute:\n\n- [Let's Verify Step by Step](https://arxiv.org/abs/2305.20050)\n  - This paper explores the idea of training a verifier model to verify solutions or logical steps, and then using that verifier to score the correctness of many LLM-generated problem solutions.\n  - The verifier models comes in two types: one that predicts the probability that each _step_ is correct, and one that predicts the probability that the entire solution as a whole is correct.\n  - The first type of verifier model requires training from human-labelled data.\n  - This paper represents an interesting paradigm shift: test-time compute (the amount of compute used for inference) can matter just as much as train-time compute.\n  - This paper acknowledges the potential of training a solution-generating model on the reward outputs from the verifier model.\n\nModel Deployment:\n\n- [Distributed Training of Deep Learning Models:\n  A Taxonomic Perspective](https://arxiv.org/abs/2007.03970)\n  - This paper describes the landscape of distributed model training (as of 2020).\n  - Distributed model training is broken down into two main categories: **data parallelism** and **model parallelism**.\n  - Data parallelism splits the training data across multiple replicas of the same model.\n    - As long as individual data points can be processed by the model independently (i.e. you aren't using something like batch normalization), then you can simply split a mini-batch of data across multiple models and then aggregate the results at the end.\n  - Model parallelism splits the model across multiple processes / machines.\n    - This is useful if the model is too large to fit on a single machine / GPU.\n    - Model parallelism can be either **horizontal** or **vertical**.\n    - Regardless of the type of model parallelism, this requires passing messages about model state between machines, where this communication can become a bottleneck.\n    - Vertical model parallelism means splitting the model layer-by-layer.\n      - Naively, all but one of the machines will be active at once unless a pipelining technique is used.\n    - Horizontal model parallelism means splitting the model within layers.\n      - This is much more complex and often avoided if possible.\n      - In a simple example, consider splitting a single large matrix multiplication across multiple machines. At the end of the day, a matrix multiplication is most simply a series of dot products.\n     \nEmbedding Models:\n- [Text and Code Embeddings by Contrastive Pre-Training](https://arxiv.org/abs/2201.10005)\n  - This paper introduces an embedding model trained with large-scale contrastive pre-training.\n  - Contrastive pre-training is where the model is tasked to produce a single vector for a segment of text. If two segments of text are assumed to be similar, then cross-entropy loss incentivizes the model to increase the cosine similarity between those two vectors. The opposite is true for negative pairs.\n  - This learning is unsupervised, leveraging the assumption that nearby text is similar.\n  - They use a clever training trick where they compute the embedding vector for every segment in a batch once, and then they use all the embeddings not related to the original positive text pair to represent the negative pairs.\n\n## Distributed Systems\n\n- [Spanner: Google’s Globally-Distributed Database](https://static.googleusercontent.com/media/research.google.com/en//archive/spanner-osdi2012.pdf)\n- [MapReduce: Simplified Data Processing on Large Clusters](https://static.googleusercontent.com/media/research.google.com/en//archive/mapreduce-osdi04.pdf)\n- [ZooKeeper: Wait-free coordination for Internet-scale systems](https://www.usenix.org/legacy/event/atc10/tech/full_papers/Hunt.pdf)\n"},"__N_SSG":true},"page":"/papers","query":{},"buildId":"m1-HDYOkaNCw9aQ2kPhKg","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>